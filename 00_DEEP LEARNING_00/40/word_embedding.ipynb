{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f69497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ffb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']\n",
    "\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e705cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 14]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot('nice food', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddbbefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 14],\n",
       " [29, 8],\n",
       " [12, 37],\n",
       " [12, 39, 36],\n",
       " [32, 35, 6],\n",
       " [47, 14],\n",
       " [15, 35, 36],\n",
       " [10, 47],\n",
       " [10, 21],\n",
       " [2, 7]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "encoded_reviews = [one_hot(d, vocab_size) for d in reviews] # giving them index for each word they will be one hot encoded automatically while training in the keras layer\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b67c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 14,  0],\n",
       "       [29,  8,  0],\n",
       "       [12, 37,  0],\n",
       "       [12, 39, 36],\n",
       "       [32, 35,  6],\n",
       "       [47, 14,  0],\n",
       "       [15, 35, 36],\n",
       "       [10, 47,  0],\n",
       "       [10, 21,  0],\n",
       "       [ 2,  7,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 3\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen = max_length, padding = 'post') # padding all sentence because all are not of same length\n",
    "padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fd0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_vec_size = 4\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeded_vec_size, input_length=3, name = 'embedding')) # the size of the dictionary(no. of total words), no. of features for each word, size of each sentence, name of the layer so we can use it later to get weights\n",
    "model.add(Flatten()) # for each sentence we will get 3 vectors of size 4(coz we have 3 words for each sentence and 4 features for each words) so joining them for training\n",
    "model.add(Dense(1, activation='sigmoid')) # 1 neuron layer for output and then back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ec1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2adaba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 4)              200       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f074a2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198b61112a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe398cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step - loss: 0.6419 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6418737173080444, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a21b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0] # this is the weights we want for each word there are four "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c87a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.06177153, -0.02295771, -0.06856312,  0.03601143], dtype=float32),\n",
       " array([ 0.00479292,  0.06162761,  0.09652323, -0.05066398], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[3], weights[37] # these are the one_hot indexes we calculated before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb64a69",
   "metadata": {},
   "source": [
    "## we dont need to predict anything because its not our real goal here, our real goal is to extract the features, weights of each word here so we can use them afterwards for diff. problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d580f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
